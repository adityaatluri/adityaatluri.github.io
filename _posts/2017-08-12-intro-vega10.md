---
title: "Intro Vega10"
layout: post
date: 2017-08-12 22:48
image: /assets/images/markdown.jpg
headerImage: false
tag:
- amd
- vega
- rx vega
category: blog
author: adityaatluri
description: What's new in Vega for Compute?
# jemoji: '<img class="emoji" title=":ramen:" alt=":ramen:" src="https://assets.github.com/images/icons/emoji/unicode/1f35c.png" height="20" width="20" align="absmiddle">'
---

## Introduction:

Vega10 or GFX9 added new and modified existing isa. We discuss about what the changes and new additions are.

#### Contents
- [Packed Math](#Packed-Math-for-FP16/U16)

---

### Packed Math for FP16/U16
GCN architectures before Vega, Fiji and Polaris do support half-precision vector ALU ops. Few of the half-precision supported by fiji are `v_add_f16`, `v_mul_f16` but, they run at the same rate of full-precision instructions. For example, a fp16 mac and a fp32 mac on fiji takes same number of cycles (gives 8TFLOPs). What previous architectures lack is able to operate on the 16-MSB (16 Most Significant Bits) of vGPRs at full rate. With Vega, not only mac operates on MSB, but can also do mac on both LSB and MSB at the same time pushing the available peak throughput to 25TFLOPs. This feature is called Rapid Packed Math, across this text we use the word Packed-Math and Rapid Packed Math interchangably. The new isa also support signed and unsigned shorts. Here are the new 16bit isa ops added to vega:

{% highlight asm %}
v_pk_mad_i16
v_pk_mul_lo_u16
v_pk_add_i16
v_pk_sub_i16
v_pk_lshlrev_b16
v_pk_lshrrev_b16
v_pk_ashrrev_i16
v_pk_max_i16
v_pk_min_i16
v_pk_mad_u16
v_pk_add_u16
v_pk_sub_u16
v_pk_max_u16
v_pk_min_u16

v_pk_fma_f16
v_pk_add_f16
v_pk_mul_f16
v_pk_min_f16
v_pk_max_f16

v_mad_mix_f32
v_mad_mixlo_f16
v_mad_mixhi_f16

s_pack_ll_b16_b32
s_pack_lh_b16_b32
s_pack_hh_b16_b32
{% endhighlight %}

For discussion, we focus on fp16 instructions. Same concepts can be extended to i16 instructions.

These instructions operate on a 32 bit register containing 2 fp16 data. Vega adds more modifiers to these instructions giving flexibility to access MSB and LSB between vgprs instead of simple vectorized op. These isa support `opsel_hi:[0,0,0] opsel:[1,1,1]` modifiers which can grab fp16 from either MSB or LSB of a register.
For example, let us try out a simple 2x2 matrix multiplication where 2 fp16s are packed into one register making up of 2 registers per matrix. For simplicity, we assume that matrix A `{a.x, a.y, a.z, a.w}` is present in `v[1:2]`, B `{b.x, b.y, b.z, b.w}` in `v[3:4]` and C `{c.x, c.y, c.z, c.w}` in `v[5:6]`

The result is,

{% highlight cpp %}
c.x = a.x * b.x + a.y * b.z + c.x;
c.y = a.x * b.y + a.y * b.w + c.y;
c.z = a.z * b.x + a.w * b.z + c.z;
c.w = a.z * b.y + a.w * b.w + c.w;
{% endhighlight %}

The following HIP kernel translates 2x2 fp16 gemm kernel to packed math isa.

{% highlight cpp %}
extern "C" half4 pk_fma_f16(half4, half4, half4) __asm("llvm.fma.v2f16");

__global__ void MatMul(half4 *A, half4 *B, half4 *C) {
  int tx = hipThreadIdx_x;
  half4 a = A[tx];
  half4 b = B[tx];
  half4 c = C[tx];

  c.xy = pk_fma_f16(a.xx, b.xy, c.xy);
  c.xy = pk_fma_f16(a.yy, b.zw, c.xy);

  c.zw = pk_fma_f16(a.zz, b.xy, c.zw);
  c.zw = pk_fma_f16(a.ww, b.zw, c.zw);

  C[tx] = c;
}
{% endhighlight %}

{% highlight shell %}
$ hipcc --amdgpu-target=gfx900 t1.cpp
{% endhighlight %}

This kernel translates to

{% highlight asm %}
v_pk_fma_f16 v5, v1, v3, v5 op_sel:[1,0,0] op_sel_hi:[1,1,1]
v_pk_fma_f16 v5, v1, v4, v5 op_sel:[0,0,0] op_sel_hi:[0,1,1]
v_pk_fma_f16 v6, v2, v3, v5 op_sel:[1,0,0] op_sel_hi:[1,1,1]
v_pk_fma_f16 v6, v2, v4, v5 op_sel:[0,0,0] op_sel_hi:[0,1,1]
{% endhighlight %}


For first 2 instructions, `opsel_hi` and `opsel` control 16bits of destination register. For first instruction, `1,1,1` in `opsel_hi` represent `v1[16:31], v3[16:31], v5[16:31]` and `1,0,0` in `opsel` represent `v1[16:31], v3[0:15], v5[0:15]`. For second instruction, `0,1,1` in `opsel_hi` represent `v1[0:15], v4[16:31], v5[16:31]` and `0,0,0` represent `v1[0:15], v4[0:15], v5[0:15]`.
A peak of 25TFLOPs can be achieved with these instructions on MI25.
ROCm stack uses clang/llvm as front end compiler to generate isa for vega. To check the correctness and whether the compiler can generate vega binaries, you can use the following command:

{% highlight shell %}
$ echo “v_pk_fma_f16 v5, v1, v3, v5 op_sel:[1,0,0] op_sel_hi:[1,1,1]” | llvm-mc -show-encoding -arch=amdgcn -mcpu=gfx900
 .text
 v_pk_fma_f16 v5, v1, v3, v5 op_sel:[1,0,0] ; encoding: [0x05,0x48,0x8e,0xd3,0x01,0x07,0x16,0x1c]
{% endhighlight %}

---




### Mixed Precision ISA
With the breakthrough of Deep Learning and the pursuit of decreasing training time, using low-precision data for training demanded hardware for low-precision math ops. But, training on just low-precision data decreased accuracy of the networks. Using mixed precision ops and data types improved performance without losing much of accuracy. Vega adds new isa for mixed precision math ops.


{% highlight asm %}
v_mad_mix_f32   dst, src0, src1, src2
v_mad_mixlo_f16 dst, src0, src1, src2
v_mad_mixhi_f16 dst, src0, src1, src2
{% endhighlight %}

 Let's try `v_mad_mix_f32`. `v_mad_mix_f32` can do multiplication-and-add on MSB or LSB or full DWORD of source registers. Just like packed instructions these isa support `opsel_hi, opsel` modifiers. The usage of `op_sel_hi` and `op_sel` is different for mixed precision ops and packed math ops. The following table shows how modifiers work:

| Bit Representation {op_sel_hi,op_sel}| Description |
|------|----|
| 2'b00 | [0:31] |
| 2'b01 | [0:31] |
| 2'b10 | [0:15] |
| 2'b11 | [16:31] |

For example, if you want to implement 2x2 matrix multiplication with A (fp16), B (fp16) and C (fp32) matrix, the code looks like this

{% highlight cpp %}
__global__ void MatMul(half4 *A, half4 *B, float4 *C) {
  int tx = hipThreadIdx_x;
  half4 a  = A[tx];
  half4 b  = B[tx];
  float4 c = C[tx];

  // c.x = a.x * b.x + a.y * b.z + c.x;
  asm volatile ("v_mad_mix_f32 %0, %1, %2, %3 op_sel_hi:[1,1,0]":"=v"(c.x):"v"(a.xy),"v"(b.xy),"v"(c.x));
  asm volatile ("v_mad_mix_f32 %0, %1, %2, %3 op_sel_hi:[1,1,0] op_sel:[1,0,0]":"=v"(c.x):"v"(a.xy),"v"(b.zw),"v"(c.x));
  // c.y = a.x * b.y + a.y * b.w + c.y;
  asm volatile ("v_mad_mix_f32 %0, %1, %2, %3 op_sel_hi:[1,1,0] op_sel:[0,1,0]":"=v"(c.y):"v"(a.xy),"v"(b.xy),"v"(c.y));
  asm volatile ("v_mad_mix_f32 %0, %1, %2, %3 op_sel_hi:[1,1,0] op_sel:[1,1,0]":"=v"(c.y):"v"(a.xy),"v"(b.zw),"v"(c.y));
  // c.z = a.z * b.x + a.w * b.z + c.z;
  asm volatile ("v_mad_mix_f32 %0, %1, %2, %3 op_sel_hi:[1,1,0]":"=v"(c.z):"v"(a.zw),"v"(b.xy),"v"(c.z));
  asm volatile ("v_mad_mix_f32 %0, %1, %2, %3 op_sel_hi:[1,1,0] op_sel:[1,0,0]":"=v"(c.z):"v"(a.zw),"v"(b.zw),"v"(c.z));
  // c.w = a.z * b.y + a.w * b.w + c.w;
  asm volatile ("v_mad_mix_f32 %0, %1, %2, %3 op_sel_hi:[1,1,0] op_sel:[0,1,0]":"=v"(c.w):"v"(a.zw),"v"(b.xy),"v"(c.w));
  asm volatile ("v_mad_mix_f32 %0, %1, %2, %3 op_sel_hi:[1,1,0] op_sel:[1,1,0]":"=v"(c.w):"v"(a.zw),"v"(b.zw),"v"(c.w));
  C[tx] = c;
}
{% endhighlight %}

{% highlight asm %}
# c.xyzw = v[1:4]
# a.xyzw = v[5:6]
# b.xyzw = v[7:8]
# c.x = a.x * b.x + c.x
v_mad_mix_f32 v1, v5, v7, v1 op_sel_hi:[1,1,0]
# c.x = a.y * b.z + c.x
v_mad_mix_f32 v1, v5, v8, v1 op_sel_hi:[1,1,0] op_sel:[1,0,0]
{% endhighlight %}

{% highlight shell %}
echo " v_mad_mix_f32 v1, v2, v3, v4 op_sel_hi:[1,1,0]" | llvm-mc -show-encoding -arch=amdgcn -mcpu=gfx900
	.text
	v_mad_mix_f32 v1, v2, v3, v4 op_sel_hi:[1,1,0] ; encoding: [0x01,0x00,0xa0,0xd3,0x02,0x07,0x12,0x1c]
{% endhighlight %}

---

### Immediate Addressing Mode for Load/Store Ops
Vega supports Immediate addressing mode which means, the load/store ops can have constants in them, there by relieving register pressure and saving alu cycles.

For example, the following code
{% highlight cpp %}
__shared__ float4 c[4];
for(int i=0;i<4;i++){
  c[i] = C[tx + i];
}
{% endhighlight %}

will be unrolled to (for gfx803)
{% highlight asm %}
flat_load_dwordx4 v[3:6], v[1:2]
v_add_u32 vcc, v1, v1, 0x10
v_addc_u32 vcc, v2, 0, v2, vcc
flat_load_dwordx4 v[7:10], v[1:2]
v_add_u32 vcc, v1, v1, 0x10
v_addc_u32 vcc, v2, 0, v2, vcc
flat_load_dwordx4 v[11:14], v[1:2]
v_add_u32 vcc, v1, v1, 0x10
v_addc_u32 vcc, v2, 0, v2, vcc
flat_load_dwordx4 v[15:18], v[1:2]
{% endhighlight %}

for vega (gfx900)
{% highlight asm %}
flat_load_dwordx4 v[3:6],   v[1:2]
flat_load_dwordx4 v[7:10],  v[1:2], offset:0x10
flat_load_dwordx4 v[11:14], v[1:2], offset:0x20
flat_load_dwordx4 v[15,18], v[1:2], offset:0x30
{% endhighlight %}}
