---
title: "Intro Vega10"
layout: post
date: 2017-08-12 22:48
image: /assets/images/markdown.jpg
headerImage: false
tag:
- amd
- vega
- rx vega
category: blog
author: adityaatluri
description: What's new in Vega for Compute?
# jemoji: '<img class="emoji" title=":ramen:" alt=":ramen:" src="https://assets.github.com/images/icons/emoji/unicode/1f35c.png" height="20" width="20" align="absmiddle">'
---

## Introduction:

Vega10 or GFX9 added new and modified existing instructions. We discuss about what the changes and new additions are.

#### Contents
- [Rapid Packed Math](#Rapid-Packed-Math-for-FP16-U16)
- [Mixed Precision](#Mixed-Precision-ISA)
- [Immediate Addressing](#Immediate-Addressing-Mode)
- [New Integer Ops](#New-Integer-Ops)

---

### Rapid-Packed Math for FP16-U16
GCN architectures before Vega, Fiji and Polaris do support half-precision vector ALU ops. Few of the half-precision supported by fiji are `v_add_f16`, `v_mul_f16` but, they run at the same rate of full-precision instructions. For example, a fp16 mac and a fp32 mac on fiji takes same number of cycles (gives 8TFLOPs). What previous architectures lack is able to operate on the 16-MSB (16 Most Significant Bits) of vGPRs at full rate. With Vega, not only mac operates on MSB, but can also do mac on both LSB and MSB at the same time pushing the available peak throughput to 25TFLOPs. This feature is called Rapid Packed Math, across this text we use the word Packed-Math and Rapid Packed Math interchangably. The new isa also support signed and unsigned shorts. Here are the new 16bit isa ops added to vega:

| Vector ALU Instruction | Description |
| ---- | ---|
|`v_pk_mad_i16` | dst.i[31:16] = src0.i[31:16] * src1.i[31:16] + src2.i[31:16] . dst.i[15:0] = src0.i[15:0] * src1.i[15:0] + src2.i[15:0] |
|`v_pk_mul_lo_u16` | dst.u[31:16] = src0.u[31:16] * src1.u[31:16] . dst.u[15:0] * src1.u[15:0] . = src0.u[15:0] |
|`v_pk_add_i16` | dst.i[31:16] = src0.i[31:16] + src1.i[31:16] . dst.i[15:0] + src1.i[15:0] . = src0.i[15:0] |
|`v_pk_sub_i16` | dst.i[31:16] = src0.i[31:16] - src1.i[31:16] . dst.i[15:0] - src1.i[15:0] . = src0.i[15:0] |
|`v_pk_lshlrev_b16` | dst.u[31:16] = src1.u[31:16] << src0.u[19:16] . dst.u[15:0] << src0.u[3:0] . = src1.u[15:0] |
|`v_pk_lshrrev_b16` | dst.u[31:16] = src1.u[31:16] >> src0.u[19:16] . dst.u[15:0] >> src0.u[3:0] . = src1.u[15:0] |
|`v_pk_ashrrev_i16` | dst.i[31:16] = src1.i[31:16] >> src0.i[19:16] . dst.i[15:0] >> src0.i[3:0] . = src1.i[15:0] |
|`v_pk_max_i16` | dst.i[31:16] = (src0.i[31:16] >= src1.i[31:16]) ? src0.i[31:16] : src1.i[31:16] . dst.i[15:0] = (src0.i[15:0] >= src1.i[15:0]) ? src0.i[15:0] : src1.i[15:0] |
|`v_pk_min_i16` | dst.i[31:16] = (src0.i[31:16] < src1.i[31:16]) ? src0.i[31:16] : src1.i[31:16] . dst.i[15:0] = (src0.i[15:0] < src1.i[15:0]) ? src0.i[15:0] : src1.i[15:0] |
|`v_pk_mad_u16` | dst.u[31:16] = src0.u[31:16] * src1.u[31:16] + src2.u[31:16] . dst.u[15:0] = src0.u[15:0] * src1.u[15:0] + src2.u[15:0] |
|`v_pk_add_u16` | dst.u[31:16] = src0.u[31:16] + src1.u[31:16] . dst.u[15:0] + src1.u[15:0] . = src0.u[15:0] |
|`v_pk_sub_u16` | dst.u[31:16] = src0.u[31:16] - src1.u[31:16] . dst.u[15:0] - src1.u[15:0] . = src0.u[15:0] |
|`v_pk_max_u16` | dst.u[31:16] = (src0.u[31:16] >= src1.u[31:16]) ? src0.u[31:16] : src1.u[31:16] . dst.u[15:0] = (src0.u[15:0] >= src1.u[15:0]) ? src0.u[15:0] : src1.u[15:0] |
|`v_pk_min_u16` | dst.u[31:16] = (src0.u[31:16] < src1.u[31:16]) ? src0.u[31:16] : src1.u[31:16] . dst.u[15:0] = (src0.u[15:0] < src1.u[15:0]) ? src0.u[15:0] : src1.u[15:0] |
|`v_pk_fma_f16` | dst.f[31:16] = src0.f[31:16] * src1.f[31:16] + src2.f[31:16] . dst.f[15:0] = src0.f[15:0] * src1.f[15:0] + src2.f[15:0] |
|`v_pk_add_f16` | dst.f[31:16] = src0.f[31:16] + src1.f[31:16] . dst.f[15:0] + src1.f[15:0] . = src0.f[15:0] |
|`v_pk_mul_f16` | dst.f[31:16] = src0.f[31:16] * src1.f[31:16] . dst.f[15:0] * src1.f[15:0] . = src0.f[15:0] |
|`v_pk_min_f16` | dst.f[31:16] = min(src0.f[31:16], src1.f[31:16]) . dst.f[15:0] = min(src0.f[15:0], src1.u[15:0]) |
|`v_pk_max_f16` | dst.f[31:16] = max(src0.f[31:16], src1.f[31:16]) . dst.f[15:0] = max(src0.f[15:0], src1.f[15:0]) |

| Scalar ALU Instruction | Description |
|------------|-----------|
| `s_pack_ll_b32_b16` | dst.u[31:0] = {s1.u[15:0], s0.u[15:0]} |
| `s_pack_lh_b32_b16` | dst.u[31:0] = {s1.u[31:16], s0.u[15:0]} |
| `s_pack_hh_b32_b16` | dst.u[31:0] = {s1.u[31:16], s0.u[31:16]} |


For discussion, we focus on fp16 instructions. Same concepts can be extended to i16 instructions.

These instructions operate on a 32 bit register containing 2 fp16 data. Vega adds more modifiers to these instructions giving flexibility to access MSB and LSB between vgprs instead of simple vectorized op. These isa support `opsel_hi:[0,0,0] opsel:[1,1,1]` modifiers which can grab fp16 from either MSB or LSB of a register.
For example, let us try out a simple 2x2 matrix multiplication where 2 fp16s are packed into one register making up of 2 registers per matrix. For simplicity, we assume that matrix A `{a.x, a.y, a.z, a.w}` is present in `v[1:2]`, B `{b.x, b.y, b.z, b.w}` in `v[3:4]` and C `{c.x, c.y, c.z, c.w}` in `v[5:6]`

The result is,

{% highlight cpp %}
c.x = a.x * b.x + a.y * b.z + c.x;
c.y = a.x * b.y + a.y * b.w + c.y;
c.z = a.z * b.x + a.w * b.z + c.z;
c.w = a.z * b.y + a.w * b.w + c.w;
{% endhighlight %}

The following HIP kernel translates 2x2 fp16 gemm kernel to packed math isa.

{% highlight cpp %}
extern "C" half4 pk_fma_f16(half4, half4, half4) __asm("llvm.fma.v2f16");

__global__ void MatMul(half4 *A, half4 *B, half4 *C) {
  int tx = hipThreadIdx_x;
  half4 a = A[tx];
  half4 b = B[tx];
  half4 c = C[tx];

  c.xy = pk_fma_f16(a.xx, b.xy, c.xy);
  c.xy = pk_fma_f16(a.yy, b.zw, c.xy);

  c.zw = pk_fma_f16(a.zz, b.xy, c.zw);
  c.zw = pk_fma_f16(a.ww, b.zw, c.zw);

  C[tx] = c;
}
{% endhighlight %}

{% highlight shell %}
$ hipcc --amdgpu-target=gfx900 t1.cpp
{% endhighlight %}

This kernel translates to

{% highlight asm %}
v_pk_fma_f16 v5, v1, v3, v5 op_sel:[1,0,0] op_sel_hi:[1,1,1]
v_pk_fma_f16 v5, v1, v4, v5 op_sel:[0,0,0] op_sel_hi:[0,1,1]
v_pk_fma_f16 v6, v2, v3, v5 op_sel:[1,0,0] op_sel_hi:[1,1,1]
v_pk_fma_f16 v6, v2, v4, v5 op_sel:[0,0,0] op_sel_hi:[0,1,1]
{% endhighlight %}


For first 2 instructions, `opsel_hi` and `opsel` control 16bits of destination register. For first instruction, `1,1,1` in `opsel_hi` represent `v1[16:31], v3[16:31], v5[16:31]` and `1,0,0` in `opsel` represent `v1[16:31], v3[0:15], v5[0:15]`. For second instruction, `0,1,1` in `opsel_hi` represent `v1[0:15], v4[16:31], v5[16:31]` and `0,0,0` represent `v1[0:15], v4[0:15], v5[0:15]`.
A peak of 25TFLOPs can be achieved with these instructions on MI25.
ROCm stack uses clang/llvm as front end compiler to generate isa for vega. To check the correctness and whether the compiler can generate vega binaries, you can use the following command:

{% highlight shell %}
$ echo “v_pk_fma_f16 v5, v1, v3, v5 op_sel:[1,0,0] op_sel_hi:[1,1,1]” | llvm-mc -show-encoding -arch=amdgcn -mcpu=gfx900
 .text
 v_pk_fma_f16 v5, v1, v3, v5 op_sel:[1,0,0] ; encoding: [0x05,0x48,0x8e,0xd3,0x01,0x07,0x16,0x1c]
{% endhighlight %}

---




### Mixed Precision ISA
With the breakthrough of Deep Learning and the pursuit of decreasing training time, using low-precision data for training demanded hardware for low-precision math ops. But, training on just low-precision data decreased accuracy of the networks. Using mixed precision ops and data types improved performance without losing much of accuracy. Vega adds new isa for mixed precision math ops.



| Mixed Precision Instructions | Description |
|---|------|
| `v_mad_mix_f32   dst, src0, src1, src2` | dst.f[31:0] = src0.f * src1.f + src2.f |
| `v_mad_mixlo_f16 dst, src0, src1, src2` | dst.f[15:0] = src0.f * src1.f + src2.f |
| `v_mad_mixhi_f16 dst, src0, src1, src2` | dst.f[31:16] = src0.f * src1.f + src2.f |

*Size and location of src0, src1 and src2 controlled by OPSEL: 0=src[31:0], 1=src[31:0], 2=src[15:0], 3=src[31:16]*

 Let's try `v_mad_mix_f32`. `v_mad_mix_f32` can do multiplication-and-add on MSB or LSB or full DWORD of source registers. Just like packed instructions these isa support `opsel_hi, opsel` modifiers. The usage of `op_sel_hi` and `op_sel` is different for mixed precision ops and packed math ops. The following table shows how modifiers work:

| Bit Representation {op_sel_hi,op_sel}| Description |
|------|----|
| 2'b00 | [0:31] |
| 2'b01 | [0:31] |
| 2'b10 | [0:15] |
| 2'b11 | [16:31] |

For example, if you want to implement 2x2 matrix multiplication with A (fp16), B (fp16) and C (fp32) matrix, the code looks like this

{% highlight cpp %}
__global__ void MatMul(half4 *A, half4 *B, float4 *C) {
  int tx = hipThreadIdx_x;
  half4 a  = A[tx];
  half4 b  = B[tx];
  float4 c = C[tx];

  // c.x = a.x * b.x + a.y * b.z + c.x;
  asm volatile ("v_mad_mix_f32 %0, %1, %2, %3 op_sel_hi:[1,1,0]":"=v"(c.x):"v"(a.xy),"v"(b.xy),"v"(c.x));
  asm volatile ("v_mad_mix_f32 %0, %1, %2, %3 op_sel:[1,0,0] op_sel_hi:[1,1,0]":"=v"(c.x):"v"(a.xy),"v"(b.zw),"v"(c.x));
  // c.y = a.x * b.y + a.y * b.w + c.y;
  asm volatile ("v_mad_mix_f32 %0, %1, %2, %3 op_sel:[0,1,0] op_sel_hi:[1,1,0]":"=v"(c.y):"v"(a.xy),"v"(b.xy),"v"(c.y));
  asm volatile ("v_mad_mix_f32 %0, %1, %2, %3 op_sel:[1,1,0] op_sel_hi:[1,1,0]":"=v"(c.y):"v"(a.xy),"v"(b.zw),"v"(c.y));
  // c.z = a.z * b.x + a.w * b.z + c.z;
  asm volatile ("v_mad_mix_f32 %0, %1, %2, %3 op_sel_hi:[1,1,0]":"=v"(c.z):"v"(a.zw),"v"(b.xy),"v"(c.z));
  asm volatile ("v_mad_mix_f32 %0, %1, %2, %3 op_sel:[1,0,0] op_sel_hi:[1,1,0]":"=v"(c.z):"v"(a.zw),"v"(b.zw),"v"(c.z));
  // c.w = a.z * b.y + a.w * b.w + c.w;
  asm volatile ("v_mad_mix_f32 %0, %1, %2, %3 op_sel:[0,1,0] op_sel_hi:[1,1,0]":"=v"(c.w):"v"(a.zw),"v"(b.xy),"v"(c.w));
  asm volatile ("v_mad_mix_f32 %0, %1, %2, %3 op_sel:[1,1,0] op_sel_hi:[1,1,0]":"=v"(c.w):"v"(a.zw),"v"(b.zw),"v"(c.w));
  C[tx] = c;
}
{% endhighlight %}

{% highlight asm %}
# c.xyzw = v[1:4]
# a.xyzw = v[5:6]
# b.xyzw = v[7:8]
# c.x = a.x * b.x + c.x
v_mad_mix_f32 v1, v5, v7, v1 op_sel_hi:[1,1,0]
# c.x = a.y * b.z + c.x
v_mad_mix_f32 v1, v5, v8, v1 op_sel:[1,0,0] op_sel_hi:[1,1,0]
{% endhighlight %}

{% highlight shell %}
$ echo "v_mad_mix_f32 v1, v5, v8, v1 op_sel:[1,0,0] op_sel_hi:[1,1,0]" | llvm-mc -show-encoding -arch=amdgcn -mcpu=gfx900
	.text
	v_mad_mix_f32 v1, v5, v8, v1 op_sel:[1,0,0] op_sel_hi:[1,1,0] ; encoding: [0x01,0x08,0xa0,0xd3,0x05,0x11,0x06,0x1c]
{% endhighlight %}

---

### Immediate Addressing Mode
Vega supports Immediate addressing mode which means, the load/store ops can have constants in them, there by relieving register pressure and saving alu cycles.

For example, the following code
{% highlight cpp %}
__shared__ float4 c[4];
for(int i=0;i<4;i++){
  c[i] = C[tx + i];
}
{% endhighlight %}

will be unrolled to (for gfx803)
{% highlight asm %}
flat_load_dwordx4 v[3:6], v[1:2]
v_add_u32 vcc, v1, v1, 0x10
v_addc_u32 vcc, v2, 0, v2, vcc
flat_load_dwordx4 v[7:10], v[1:2]
v_add_u32 vcc, v1, v1, 0x10
v_addc_u32 vcc, v2, 0, v2, vcc
flat_load_dwordx4 v[11:14], v[1:2]
v_add_u32 vcc, v1, v1, 0x10
v_addc_u32 vcc, v2, 0, v2, vcc
flat_load_dwordx4 v[15:18], v[1:2]
{% endhighlight %}

for vega (gfx900)
{% highlight asm %}
flat_load_dwordx4 v[3:6],   v[1:2]
flat_load_dwordx4 v[7:10],  v[1:2], offset:0x10
flat_load_dwordx4 v[11:14], v[1:2], offset:0x20
flat_load_dwordx4 v[15,18], v[1:2], offset:0x30
{% endhighlight %}

Testing syntax using llvm assembler for both gfx803 and gfx900,
{% highlight shell %}
$ echo "flat_load_dwordx4 v[15:18], v[1:2], offset:0x20" | llvm-mc -show-encoding -arch=amdgcn -mcpu=gfx900
	.text
	flat_load_dwordx4 v[15:18], v[1:2] offset:32 ; encoding: [0x20,0x00,0x5c,0xdc,0x01,0x00,0x00,0x0f]

$ echo "flat_load_dwordx4 v[15:18], v[1:2], offset:0x20" | llvm-mc -show-encoding -arch=amdgcn -mcpu=gfx803
	.text
<stdin>:1:1: error: invalid operand for instruction
flat_load_dwordx4 v[15:18], v[1:2], offset:0x20
^

{% endhighlight %}
---

### New Integer ops
Vega adds new integer 32bit ops.

| Instruction | Description |
|-------------|----------------|
| `v_xad_u32 dst, src0, src1, src2` | dst.u32 = src0.u32 ^ src1.u32 + src2.u32 |
| `v_lshl_add_u32 dst, src0, src1, src2` | dst.u32 = (src0.u32 << src1.u32[4:0]) + src2.u32|
| `v_add_lshl_u32 dst, src0, src1, src2`| dst.u32 = (src0.u32 + src1.u32) << src2.u32[4:0] |
| `v_add3_u32 dst, src0, src1, src2` | dst.u32 = src0.u32 + src1.u32 + src2.u32 |
| `v_lshl_or_b32 dst, src0, src1, src2` | dst.u32 = (src0.u32 << src1.u32[4:0]) \| src2.u32 |
| `v_and_or_b32 dst, src0, src1, src2` | dst.u32 = (src0.u32 & src1.u32) \| src2.u32|
| `v_or3_b32 dst, src0, src1, src2` | dst.u32 = (src0.u32 \| src1.u32 \| src2.u32)|

Latest LLVM Assembler supports the new integer ops

{% highlight shell %}
$ echo "v_xad_u32 v0, v1, v2, v3" | llvm-mc --show-encoding -arch=amdgcn -mcpu=gfx900
	.text
	v_xad_u32 v0, v1, v2, v3        ; encoding: [0x00,0x00,0xf3,0xd1,0x01,0x05,0x0e,0x04]
{% endhighlight %}

{% highlight shell %}
$ echo "v_lshl_add_u32 v0, v1, v2, v3" | llvm-mc --show-encoding -arch=amdgcn -mcpu=gfx900
	.text
	v_lshl_add_u32 v0, v1, v2, v3   ; encoding: [0x00,0x00,0xfd,0xd1,0x01,0x05,0x0e,0x04]
{% endhighlight %}

{% highlight shell %}
$ echo "v_add_lshl_u32 v0, v1, v2, v3" | llvm-mc --show-encoding -arch=amdgcn -mcpu=gfx900
	.text
	v_add_lshl_u32 v0, v1, v2, v3   ; encoding: [0x00,0x00,0xfe,0xd1,0x01,0x05,0x0e,0x04]
{% endhighlight %}

{% highlight shell %}
$ echo "v_add3_u32 v0, v1, v2, v3" | llvm-mc --show-encoding -arch=amdgcn -mcpu=gfx900
	.text
	v_add3_u32 v0, v1, v2, v3       ; encoding: [0x00,0x00,0xff,0xd1,0x01,0x05,0x0e,0x04]
{% endhighlight %}

{% highlight shell %}
$ echo "v_lshl_or_b32 v0, v1, v2, v3" | llvm-mc --show-encoding -arch=amdgcn -mcpu=gfx900
	.text
	v_lshl_or_b32 v0, v1, v2, v3    ; encoding: [0x00,0x00,0x00,0xd2,0x01,0x05,0x0e,0x04]
{% endhighlight %}

{% highlight shell %}
$ echo "v_and_or_b32 v0, v1, v2, v3" | llvm-mc --show-encoding -arch=amdgcn -mcpu=gfx900
	.text
	v_and_or_b32 v0, v1, v2, v3     ; encoding: [0x00,0x00,0x01,0xd2,0x01,0x05,0x0e,0x04]
{% endhighlight %}

{% highlight shell %}
$ echo "v_or3_b32 v0, v1, v2, v3" | llvm-mc --show-encoding -arch=amdgcn -mcpu=gfx900
	.text
	v_or3_b32 v0, v1, v2, v3        ; encoding: [0x00,0x00,0x02,0xd2,0x01,0x05,0x0e,0x04]
{% endhighlight %}
